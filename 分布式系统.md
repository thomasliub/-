# 1. 分布式系统特征
* 网络通信
* 异构型
	* 网络
	* 计算硬件
	* 操作系统
	* 编程语言，环境
* 开放性
	* 新的资源能够被增加和客户使用的程度
* 安全性
	* 机密性
	* 完整性
	* 可用性
* 可伸缩性
	* 系统资源的可伸缩
	* 控制物理资源的开销
	* 控制性能损失
	* 防止软件资源用尽
	* 避免性能瓶颈
* 故障处理
* 并发性

# 2. 系统模型：交互模型，故障模型，安全模型
* 交互模型
	* 解决分布式系统性能问题，
	* 解决分布式系统中设置时间约束的困难
	* 考虑系统元素间通信的结构和顺序
* 故障模型
	* 给出进程和通信通道故障的精确规约
	* 定义可靠的通信和正确的进程
	* 考虑系统错误的操作方式
* 安全模型
	* 讨论对进程和通信通道的各种威胁
	* 考虑如何保护系统不受错误操作干扰和窃取数据

# 3. 通信范型
* 进程间通信
	* 消息
	* socket
* 远程调用
	* RPC,RMI
* 间接通信
	* 空间解耦
	* 时间解耦
	* 组通信，发布/订阅，消息队列，分布式共享内存

# 4. 悲观锁和乐观锁
* 悲观锁(Pessimistic Lock)
 顾名思义，就是很悲观，每次去操作数据的时候都认为别人会修改，所以每次在操作数据的时候都会上锁，这样别人想操作这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

* 乐观锁(Optimistic Lock)
 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。

乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。

# 5. 分布式调度器
http://geek.csdn.net/news/detail/74234
![](http://img.blog.csdn.net/20160516095146051)
## 5.1 中央式调度
* Google Borg
![](http://img.blog.csdn.net/20160516095454584)

* Kubernetes
![](http://img.blog.csdn.net/20160516100137493)

* Docker Swarm
![](http://img.blog.csdn.net/20160516100214034)

* 腾讯 Torca
![](http://img.blog.csdn.net/20160516100411255)

* 阿里飞天伏羲
https://lingyun.aliyun.com/4/tech-fuxi.html
![](http://img.blog.csdn.net/20160516100451541)

## 5.2 双层调度
* Apache Mesos
![](http://img.blog.csdn.net/20160516100602071)

* Apache Hadoop YARN
http://blog.csdn.net/suifeng3051/article/details/49486927
![](http://img.blog.csdn.net/20160516101005637)
<br>
![](http://img.blog.csdn.net/20160516101401534)
![](http://img.blog.csdn.net/20151029092726524)


1. Container
Container是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。
2. Node Manager
NodeManager进程运行在集群中的节点上，每个节点都会有自己的NodeManager。NodeManager是一个slave服务：它负责接收ResourceManager的资源分配请求，分配具体的Container给应用。同时，它还负责监控并报告Container使用信息给ResourceManager。
    - 接收ResourceManager的请求，分配Container给应用的某个任务
    - 和ResourceManager交换信息以确保整个集群平稳运行。ResourceManager就是通过收集每个NodeManager的报告信息来追踪整个集群健康状态的，而NodeManager负责监控自身的健康状态。
    - 管理每个Container的生命周期
    - 管理每个节点上的日志
    - 执行Yarn上面应用的一些额外的服务，比如MapReduce的shuffle过程

3. Resource Manager
ResourceManager主要有两个组件：Scheduler和ApplicationManager。
Scheduler的角色是一个纯调度器，它只负责调度Containers，不会关心应用程序监控及其运行状态等信息。
Capacity Scheduler和Fair Scheduler

4. Application Master
ApplicationMaster的主要作用是向ResourceManager申请资源并和NodeManager协同工作来运行应用的各个任务然后跟踪它们状态及监控各个任务的执行，遇到失败的任务还负责重启它。

* 这个设计大大减小了 ResourceManager 的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。
* 在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 AppMst，让更多类型的编程模型能够跑在 Hadoop 集群中，可以参考 hadoop Yarn 官方配置模板中的 ``mapred-site.xml`` 配置。
* 对于资源的表示以内存为单位 ( 在目前版本的 Yarn 中，没有考虑 cpu 的占用 )，比之前以剩余 slot 数目更合理。
* 老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsManager，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。
* Container 是 Yarn 为了将来作资源隔离而提出的一个框架。这一点应该借鉴了 Mesos 的工作，目前是一个框架，仅仅提供 java 虚拟机内存的隔离 ,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制 , 既然资源表示成内存量，那就没有了之前的 map slot/reduce slot 分开造成集群资源闲置的尴尬情况。

http://blog.csdn.net/suifeng3051/article/details/49486927

作业方式：
* 内存作业(spark)
* 离线作业
* 流式作业(storm)
* 迭代式作业(iMapReduce)
* crawler server
* web server等，不同任务对资源有不同需求。

## 5.3 共享状态调度
* Google Omega
![]()

* 微软Apollo
![](http://img.blog.csdn.net/20160516101919219)

# 6. 分布式系统的两种方式
* 水平扩展
* 垂直扩展

# 7. zookeeper
![](http://www.aboutyun.com/data/attachment/forum/201608/20/184447i91rsvg573m888f5.png)
* znode类型
    1. 持久化目录节点
    2. 持久化目录节点，顺序编号节点
    3. 临时目录节点
    4. 临时顺序编号目录节点

## 7.1 zookeeper功能
https://www.cnblogs.com/felixzh/p/5869212.html
1.命名服务
2.配置管理
![](http://www.aboutyun.com/data/attachment/forum/201608/20/184509blnln2a7n5qqa95s.png)
3.集群管理   
![](http://www.aboutyun.com/data/attachment/forum/201608/20/184530b6abmegbk09ffgva.png)
4.分布式锁  
锁服务可以分为两类，一个是保持独占，另一个是控制时序
![](http://www.aboutyun.com/data/attachment/forum/201608/20/184557iv77xzbyas7bc99o.png)
5.队列管理
    两种类型的队列：

    1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 
    
    2、队列按照 FIFO 方式进行入队和出队操作。 
    
    第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 
    
    第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。

## 7.2 Fast Paxos选主流程
节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。Paxos 算法就是一种基于消息传递模型的一致性算法。
1. 第一阶段
    1. Proposer发起Prepare请求给**大部分**acceptors,带着**提案编号n**
    2. Acceptor回复
        * 只有提案号**大于AcceptorMaxN**时，提案才被接受，并更新acceptor的**AcceptorMaxN = n**, 否则拒绝
        * 如接受，且无已经accept的提案值，回复**(n, Null, Null)**
        * 如接受，且有已经accept的提案值，则回复并携带已接受的AcceptedMaxV, **(n, 提案值AcceptedMaxV,该提案号AcceptedMaxN**）
2. 第二阶段
    1. Proposer:
        1. 如Proposer接收到**大部分Acceptor的回复**，进入第二阶段，否则，增加n重新进入第一阶段
        2. 如所有回复都是(n, Null, Null)，则由Proposer任选V，回复(n, ProposorV)
        3. 如有至少一个回复已接受的AcceptedMaxV, 则Proposer选择最大AcceptedMaxN'的AcceptedMaxV', 回复(n, AcceptedMaxV')
    2. Acceptor:
        1. Acceptor接收到accept(n, ProposorV)
        2. 如**AcceptorMaxN > n, 拒绝**
        3. 如**AcceptorMaxN = n, 则接受**，**AcceptedMaxV=ProposorV, AcceptedMaxN=n**

Phase 1

(a) A proposer selects a proposal number n and sends a prepare request with number n to a majority of acceptors.

(b) If an acceptor receives a prepare request with number n greater than that of any prepare request to which it has already responded, then it responds to the request with a promise not to accept any more proposals numbered less than n and with the highest-numbered pro-posal (if any) that it has accepted.

Phase 2

(a) If the proposer receives a response to its prepare requests (numbered n) from a majority of acceptors, then it sends an accept request to each of those acceptors for a proposal numbered n with a value v , where v is the value of the highest-numbered proposal among the responses, or is any value if the responses reported no proposals.

(b) If an acceptor receives an accept request for a proposal numbered n, it accepts the proposal unless it has already responded to a prepare request having a number greater than n.

## 7.3 Zab协议
http://blog.jobbole.com/104985/
原子广播
Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）
Zookeeper 客户端会随机连接到 Zookeeper 集群的一个节点，如果是读请求，就直接从当前节点中读取数据；如果是写请求，那么节点就会向 leader 提交事务，leader 会广播事务，只要有超过半数节点写入成功，该写请求就会被提交（类 2PC 协议）

ZAB 中的节点有三种状态
* following：当前节点是跟随者，服从 leader 节点的命令
* leading：当前节点是 leader，负责协调事务
* election/looking：节点处于选举状态

名词解释：
* zxid
在 ZAB 协议的事务编号 Zxid 设计中，Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的ZXID，并从中读取 epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。

* epoch
可以理解为当前集群所处的年代或者周期，每个 leader 就像皇帝，都有自己的年号，所以每次改朝换代，leader 变更之后，都会在前一个年代的基础上加 1。这样就算旧的 leader 崩溃恢复之后，也没有人听他的了，因为 follower 只听从当前年代的 leader 的命令。

* quorum：集群中超过半数的节点集合

四个阶段：
Follower: AcceptedEpoch, state
1. Phase 0: Leader election（选举阶段）
    1. 只要有一个节点得到超半数节点的票数，它就可以当选准 leader
    2. 目的是就是为了选出一个准 leader，然后进入下一个阶段

2. Phase 1: Discovery（发现阶段）
    1. followers 跟准 leader 进行通信，让准leader发现当前大多数节点接收的最新提议
    2. 准 leader 生成新的 epoch，让 followers 接受，更新它们的 acceptedEpoch
    3. 一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f 在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入 Phase 0。
![](http://7xjtfr.com1.z0.glb.clouddn.com/phase1.png)

3. Phase 2，Synchronization（同步阶段）
    1. 同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本，只有当 quorum 都同步完成，准 leader 才会成为真正的 leader
    2. follower 只会接收 zxid 比自己的 lastZxid 大的提议
![](http://7xjtfr.com1.z0.glb.clouddn.com/phase2.png)

4. Phase 3: Broadcast（广播阶段）
    1. eader 可以进行消息广播
    2. 如果有新的节点加入，还需要对新节点进行同步。
![](http://7xjtfr.com1.z0.glb.clouddn.com/phase3.png)

# 8 Gossip
Gossip是一个带冗余的容错算法，更进一步，Gossip是一个最终一致性算法。虽然无法保证在某个时刻所有节点状态一致，但可以保证在”最终“所有节点一致，”最终“是一个现实中存在，但理论上无法证明的时间点。
因为Gossip不要求节点知道所有其他节点，因此又具有去中心化的特点，节点之间完全对等，不需要任何的中心节点。实际上Gossip可以用于众多能接受“最终一致性”的领域：失败检测、路由同步、Pub/Sub、动态负载均衡。
Anti-Entropy模式有完全的容错性，但有较大的网络、CPU负载；Rumor-Mongering模式有较小的网络、CPU负载，但必须为数据定义”最新“的边界，并且难以保证完全容错，对失败重启且超过”最新“期限的节点，无法保证最终一致性，或需要引入额外的机制处理不一致性。我们后续着重讨论Anti-Entropy模式的优化。
Gossip是一种去中心化、容错而又最终一致性的绝妙算法，其收敛性不但得到证明还具有指数级的收敛速度。使用Gossip的系统可以很容易的把Server扩展到更多的节点，满足弹性扩展轻而易举。

唯一的缺点是收敛是最终一致性，不使用那些强一致性的场景，比如2pc。

# 9. 分布式存储
结构化数据一般存储在关系数据库中的二维表中。
# 9.1 非结构化数据
包括所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等,一般存储在分布式文件存储系统中，如Google的 GFS中。
![](https://images2015.cnblogs.com/blog/719869/201604/719869-20160405111501703-836655618.jpg)
![](https://images2015.cnblogs.com/blog/719869/201604/719869-20160405111625140-509811189.jpg)
GFS将整个系统分为三类角色：Client（客户端）、Master（主服务器）、Chunk Server（数据块服务器）。

* Client（客户端）：是GFS提供给应用程序的访问接口，它是一组专用接口，不遵守POSIX规范，以库文件的形式提供。应用程序直接调用这些库函数，并与该库链接在一起。

* Master（主服务器）：是GFS的管理节点，主要存储与数据文件相关的元数据，而不是Chunk（数据块）。元数据包括：命名空间（Name Space），也就是整个文件系统的目录结构，一个能将64位标签映射到数据块的位置及其组成文件的表格，Chunk副本位置信息和哪个进程正在读写特定的数据块等。还有Master节点会周期性地接收从每个Chunk节点来的更新（"Heart- beat"）来让元数据保持最新状态。

* Chunk Server（数据块服务器）：负责具体的存储工作，用来存储Chunk。GFS将文件按照固定大小进行分块，默认是64MB，每一块称为一个Chunk（数据块），每一个Chunk以Block为单位进行划分，大小为64KB，每个Chunk有一个唯一的64位标签。GFS采用副本的方式实现容错，每一个Chunk有多个存储副本（默认为三个）。 Chunk Server的个数可有有多个，它的数目直接决定了GFS的规模。

HDFS，MooseFS，MogileFS等都是基于GFS.

## 9.2 半结构化数据,NoSQL
就是介于完全结构化数据（如关系型数据库、面向对象数据库中的数据）和完全无结构的数据（如声音、图像文件等）之间的数据， 半结构化数据模型具有一定的结构性，但较之传统的关系和面向对象的模型更为灵活。半结构数据模型完全不基于传统数据库模式的严格概念，这些模型中的数据都是自描述的。

被称作下一代的数据库，具有非关系型，分布式，轻量级，支持水平扩展且一般不保证遵循ACID原则的数据储存系统。“NoSQL”其实是具有误导性的别名，称作Non Relational Database(非关系型数据库)更为恰当。所谓“非关系型数据库”指的是：

* 使用松耦合类型、可扩展的数据模式来对数据进行逻辑建模(Map，列，文档，图表等)，而不是使用固定的关系模式元组来构建数据模型。

* 以遵循于CAP定理（能保证在一致性，可用性和分区容忍性三者中中达到任意两个）的跨多节点数据分布模型而设计，支持水平伸缩。这意味着对于多数据中心和动态供应（在生产集群中透明地加入/删除节点）的必要支持，也即弹性(Elasticity)。

* 拥有在磁盘或内存中，或者在这两者中都有的，对数据持久化的能力，有时候还可以使用可热插拔的定制存储。

* 支持多种的‘Non-SQL’接口(通常多于一种)来进行数据访问。

![](https://images2015.cnblogs.com/blog/719869/201604/719869-20160405111910781-2103447981.jpg)

* 接口：REST (HBase，CouchDB，Riak等)，MapReduce (HBase，CouchDB，MongoDB，Hypertable等)，Get/Put (Voldemort，Scalaris等)，Thrift (HBase，Hypertable，Cassandra等)，语言特定的API(MongoDB)。

* 逻辑数据模型：面向键值对的(Voldemort，Dynomite 等)，面向Column Family的(BigTable，HBase，Hypertable 等)，面向文档的(Couch DB，MongoDB等)，面向图的(Neo4j， Infogrid等)

* 数据分布模型：致性和可用性(HBase，Hypertable， MongoDB等)， 可用性和可分区性(Cassandra等)。一致性和可分区性的组合会导致一些非额定的节点产生可用性的损失。有趣的是目前还没有一个“非关系型数据库”支持这一组合。

* 数据持久性：基于内存的(如Redis，Scalaris， Terrastore)，基于磁盘的(如MongoDB，Riak等)，或内存及磁盘二者的结合(如 HBase，Hypertable，Cassandra)。存储的类型有助于我们辨别该解决方案适用于哪种类型。然而，在大多数情况下人们发现基于组合方 案的解决方案是最佳的选择。既能通过内存数据存储支持高性能，又能在写入足够多的数据后存储到磁盘来保证持续性。

## 9.3 分布式块存储
裸盘级共享，将物理上独立的磁盘通过SAN连接，提供RAID和LVM给主机操作系统访问，操作系统不知道下面物理磁盘分配情况。
如三个裸盘各自容量为3G, 划分为三个3个G的虚拟盘，这3个G由三个物理磁盘各自贡献1个G组成。
* 提供高性能，并发访问磁盘IO
* 逻辑盘需要使用方操作系统格式化，并完全控制，不能共享
* 不同文件系统不能共享

## 9.4 文件共享
为解决分布式块存储方案中，不能共享的问题。此方案中，共享的不是裸盘，而是通过文件共享协议访问，如FTP等。
文件共享方案中，文件元数据位于众多文件块的尾部，当需要读取文件时，需要一个IO串行访问，无法利用并行IO访问机制。
* 网络采用以太网，访问速度慢
* 无法扩展，通常单点提供服务

## 9.5 对象存储
解决上述两种方案的问题。一般提供大型文件存储。对象存储方案由三部分组成， 客户端，元数据服务器和数据服务器。
客户端提起对象访问请求，首先访问元数据服务器，获知对象数据属性，如所有数据块的存储位置；其次，利用分布式并行访问数据服务器获取对象数据；最后，在拼接成所需要的对象数据。

# 10 阶段提交
http://blog.jobbole.com/95632/
1. 2阶段提交
Master侧有超时，超时则abort commit。
Slave侧，无超时，容易造成事务阻塞。
![](http://www.hollischuang.com/wp-content/uploads/2015/12/success.png)
![](http://www.hollischuang.com/wp-content/uploads/2015/12/fail.png)

2. 3阶段提交
在slave侧引入超时机制，超时则执行commit.
![](http://www.hollischuang.com/wp-content/uploads/2015/12/3.png)

# 11 IO复用技术, epoll Vs select
http://blog.csdn.net/ccjhdopc/article/details/6435181
1. epoll突破了单进程所能打开文件描述符限制， 远超过1~2K
2. epoll调用，epoll_create(), epoll_ctrl(), epoll_wait()
    1. int epoll_create(int size)
    创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽
    2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)
    配置监听fd以及，监听哪些事件，内核既开始通过callback监视有数据的IO.并构造数据
    3. epoll_wait()待应用调用epoll_wait()返回活跃的IO数量
不像select(),在调用时才扫描IO，epoll在ctrl后就内核就开始扫描。
3. 内核和应用程序通信采用mmap, 减少拷贝
4. epoll可配置采用电平触发或者边沿触发，缺省电平触发
    1. 电平触发安全，容易实现，效率略差
    2. 边沿触发，应用控制不好，如消息没收全则不会再有事件产生， nginx采用边沿触发。

# 11. RPC调用
RMI Java的RPC实现，借鉴Google的 Protocol Buffer。


# 12. HDFS
http://www.cnblogs.com/wxisme/p/6266267.html
https://www.cnblogs.com/wxisme/p/6270860.html
HDFS是一个文件系统，用于存储和管理文件，通过统一的命名空间（类似于本地文件系统的目录树）。是分布式的，服务器集群中各个节点都有自己的角色和职责。
    * HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，之前的版本中是64M。
    
    * HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data
    
    * 目录结构及文件分块位置信息(元数据)的管理由namenode节点承担，namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的数据块信息（blockid及所在的datanode服务器）
    
    * 文件的各个block的存储管理由datanode节点承担，datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication，默认是3）
    
    * Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量，HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行。
    
    * HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改。需要频繁的RPC交互，写入性能不好。


1. 硬件错误是常态，而非异常情况，HDFS可能是有成百上千的server组成，任何一个组件都有可能一直失效，因此错误检测和快速、自动的恢复是HDFS的核心架构目标。
2. 跑在HDFS上的应用与一般的应用不同，它们主要是以流式读为主，做批量处理；比之关注数据访问的低延迟问题，更关键的在于数据访问的高吞吐量。
3. HDFS以支持大数据集合为目标，一个存储在上面的典型文件大小一般都在千兆至T字节，一个单一HDFS实例应该能支撑数以千万计的文件。
4. HDFS应用对文件要求的是write-one-read-many访问模型。一个文件经过创建、写，关闭之后就不需要改变。这一假设简化了数据一致性问 题，使高吞吐量的数据访问成为可能。典型的如MapReduce框架，或者一个web crawler应用都很适合这个模型。
5. 移动计算的代价比之移动数据的代价低。一个应用请求的计算，离它操作的数据越近就越高效，这在数据达到海量级别的时候更是如此。将计算移动到数据附近，比之将数据移动到应用所在显然更好，HDFS提供给应用这样的接口。
6. 在异构的硬件和软件平台上的可移植性。

* namenode 和datanode
![](https://images2015.cnblogs.com/blog/735119/201701/735119-20170109205921603-1216646985.gif)
HDFS采用master/slave架构。
一个HDFS集群是有一个Namenode和一定数目的Datanode组成。
Namenode是一个中心服务器，负责管理文件系统的namespace和客户端对文件的访问。Datanode在集群中一般是一个节点一个，负责管理节点上它们附带的存储

* namespace
HDFS支持传统的层次型文件组织结构

* 数据复制
    * 文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的
    * Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。
    * 块状态报告包含了一个该Datanode上所有数据块的列表
    
* 副本存放
    * HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率
    * 副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上
    * 为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本
* 安全模式
  
* 元数据的持久化和磁盘错误
    * Namenode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，Namenode都会使用一种称为EditLog的事务日志记录下来。
    * 在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示
    * FsImage和Editlog是HDFS的核心数据结构，Namenode可以配置成支持维护多个FsImage和Editlog的副本
    1. 创建文件，修改文件元数据，-> 在EditLog中增加事务日志
    2. 定期或者当log数量触发时，namenode触发由secondnamenode 负责EventLog写盘，事务提交
    3. HDFS启动，读取FsImage（保存文件系统名字空间，数据块到文件映射，文件属性等元数据）文件到内存；读取EditLog事务文件到内存，加载事务到内存FsImage并删除事务日志；FsImage存盘。叫做checkpoint
    
* 数据块&客户端缓存&流水线复制
    * 64M
    * 一次写入多次读取
    * Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个
* 文件的删除和恢复
    * HDFS会将这个文件重命名转移到/trash目录
    * 只要文件还在/trash目录中，该文件就可以被迅速地恢复
    * 文件在/trash中保存的时间是可配置的
    * 当一个文件的副本系数被减小后，Namenode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。

## 12.1 NameNode元数据管理原理分析, namenode对元数据有三种存储方式：
HDFS不适合存储小文件的原因，每个文件都会产生元信息，当小文件多了之后元信息也就多了，对namenode会造成压力。
* 内存元数据(NameSystem)
* 磁盘元数据镜像文件(FsImage)
* 数据操作日志文件（可通过日志运算出元数据）EditLog

内存元数据就是当前namenode正在使用的元数据，是存储在内存中的。
磁盘元数据镜像文件是内存元数据的镜像，保存在namenode工作目录中，它是一个准元数据，作用是在namenode宕机时能够快速较准确的恢复元数据。称为fsimage。
数据操作日志文件是用来记录元数据操作的，在每次改动元数据时都会追加日志记录，如果有完整的日志就可以还原完整的元数据。主要作用是用来完善fsimage，减少fsimage和内存元数据的差距。称为editslog。

checkpoint机制分析:
因为namenode本身的任务就非常重要，为了不再给namenode压力，日志合并到fsimage就引入了另一个角色secondarynamenode。secondarynamenode负责定期把editslog合并到fsimage，“定期”是namenode向secondarynamenode发送RPC请求的，是按时间或者日志记录条数为“间隔”的，这样即不会浪费合并操作又不会造成fsimage和内存元数据有很大的差距。因为元数据的改变频率是不固定的。

每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）。
![](https://images2015.cnblogs.com/blog/735119/201701/735119-20170110204113760-878597442.png)

## 12.2 HDFS写数据分析
* 请求和应答是使用RPC的方式，客户端通过ClientProtocol与namenode通信，namenode和datanode之间使用DatanodeProtocol交互。在设计上，namenode不会主动发起RPC，而是响应来自客户端或 datanode 的RPC请求。客户端和 datanode之间是使用socket进行数据传输，和namenode之间的交互采用nio封装的RPC。
* 在流式复制时如果有一台或两台（不是全部）没有复制成功，不影响最后结果，只不过datanode会定期向namenode汇报自身信息。如果发现异常namenode会指挥datanode删除残余数据和完善副本。如果副本数量少于某个最小值就会进入安全模式
![](https://images2015.cnblogs.com/blog/735119/201701/735119-20170110163732385-1159880889.png)

## 12.3 HDFS读数据分析
![](https://images2015.cnblogs.com/blog/735119/201701/735119-20170110180344666-218250480.png)

## 12.4 HDFS删除数据分析
1. 客户端向namenode发起RPC调用，请求删除文件。namenode检查合法性。
2. namenode查询文件相关元信息，向存储文件数据块的datanode发出删除请求。
3. datanode删除相关数据块。返回结果。
4. namenode返回结果给客户端。

# 13 Yarn 调度器
FIFO Scheduler ，Capacity Scheduler，FairS cheduler
